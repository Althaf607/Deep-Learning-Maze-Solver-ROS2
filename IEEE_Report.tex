\documentclass[conference]{IEEEtran}
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{hyperref}

\begin{document}

\title{Robust Autonomous Maze Navigation using Twin Delayed DDPG with Teacher-Student Curriculum Learning\\
\thanks{This work was conducted as a focused case study on Deep Reinforcement Learning in Robotics.}
}

\author{\IEEEauthorblockN{Althaf Ahamed}
\IEEEauthorblockA{\textit{Lead AI Engineer \& Researcher} \\
\textit{Department of Robotics and Autonomous Systems}\\
althaf607@gmail.com}
}

\maketitle

\begin{abstract}
Autonomous navigation in unknown, complex environments remains a significant challenge in mobile robotics. Traditional methods relying on simultaneous localization and mapping (SLAM) often require computationally expensive global path planning. Deep Reinforcement Learning (DRL) offers a promising end-to-end alternative, mapping sensor inputs directly to control actions. However, DRL agents suffer from poor sample efficiency and instability during the initial exploration phase, often leading to frequent collisions and slow convergence. This paper presents a robust navigation system based on the Twin Delayed Deep Deterministic Policy Gradient (TD3) algorithm. To overcome exploration challenges, we introduce a \textit{Teacher-Student} curriculum learning framework (DAGGER-style), where a geometric expert controller initially guides the learning process. Furthermore, we implement a specialized ``Goal Vector'' state representation that decouples local obstacle avoidance from global pathfinding. Experimental results in high-fidelity Gazebo simulations demonstrate that our approach achieves a 100\% success rate in maze traversal with an average velocity of 0.45 m/s, significantly outperforming standard baseline agents in both speed and safety.
\end{abstract}

\begin{IEEEkeywords}
Deep Reinforcement Learning, Mobile Robotics, TD3, Autonomous Navigation, Teacher-Student Learning, ROS 2
\end{IEEEkeywords}

\section{Introduction}
The capability to navigate efficiently and safely in unstructured environments is a prerequisite for autonomously operating mobile robots, ranging from warehouse logistics to search-and-rescue operations \cite{b1}. Classic approaches typically decompose the problem into mapping, localization, and path planning. While effective, these methods are sensitive to sensor noise and dynamic changes in the environment.

Recent advancements in Deep Reinforcement Learning (DRL) have demonstrated the potential for learning navigation policies directly from raw sensor data \cite{b2}. Algorithms such as Deep Q-Networks (DQN) and Deep Deterministic Policy Gradient (DDPG) allow robots to learn complex behaviors through trial and error. However, applying these methods to continuous control tasks in robotics introduces non-trivial difficulties: (1) The ``sparse reward'' problem, where the robot rarely reaches the goal by random chance; (2) The safety criticality, where random exploration leads to physical damage (simulated or real); and (3) The overestimation bias in value-based DRL methods, which can lead to suboptimal policies.

In this work, we propose a comprehensive navigation framework for the differential drive robot (TurtleBot3). Our contributions are threefold:
\begin{itemize}
    \item Implementation of the \textbf{Twin Delayed DDPG (TD3)} algorithm adapted for continuous motion control, mitigating the value overestimation of DDPG.
    \item Integration of a \textbf{Teacher-Student Curriculum}, utilizing a robust geometric controller to generate expert demonstrations for the initial training phase, solving the sparse reward problem.
    \item A hybrid \textbf{Safety Layer} that overrides neural network actions if imminent collision is detected, ensuring safe exploration.
\end{itemize}

\section{Related Work}
\subsection{Map-Based Navigation}
Traditional stacks like the ROS Navigation Stack utilize the Dynamic Window Approach (DWA) or Time Elastic Band (TEB) planners on top of a Costmap. While robust, they depend heavily on precise localization (AMCL) and global maps.

\subsection{Deep Reinforcement Learning in Robotics}
Deep Q-Networks (DQN) have been applied to discrete navigation tasks \cite{b3}. However, discretization of smooth motion leads to jerky trajectories. DDPG extended Q-learning to continuous action spaces, but suffers from instability. TD3 \cite{b4} addresses this by employing clipped double-Q learning and delayed policy updates, making it the state-of-the-art for continuous control tasks like locomotion and navigation.

\section{Methodology}

\subsection{Problem Formulation}
We model the navigation task as a Markov Decision Process (MDP) defined by the tuple $(S, A, R, P, \gamma)$. At each timestep $t$, the robot observes state $s_t$, executes action $a_t$, receives reward $r_t$, and transitions to $s_{t+1}$. The goal is to learn a policy $\pi(s)$ that maximizes the expected cumulative reward $J = \mathbb{E}[\sum \gamma^t r_t]$.

\subsection{State Space ($S$)}
To allow the network to generalize to unseen mazes, we maximize the use of local sensor data. The state vector is 28-dimensional:
\begin{equation}
S_t = [L_1, L_2, ..., L_{24}, v, \omega, d_{goal}, \theta_{goal}]
\end{equation}
Where:
\begin{itemize}
    \item $L_{1...24}$: 24-sector averaged LiDAR range readings (360-degree coverage reduced to 24 beams for dimensionality reduction).
    \item $v, \omega$: Current linear and angular velocities.
    \item $d_{goal}, \theta_{goal}$: Distance and relative angle to the visual checkpoint (Red Exit).
\end{itemize}

\subsection{Action Space ($A$)}
The action space is continuous:
\begin{equation}
a_t = [v_{cmd}, \omega_{cmd}]
\end{equation}
Where $v_{cmd} \in [0, 0.5] m/s$ and $\omega_{cmd} \in [-1.0, 1.0] rad/s$. This continuous space allows for smooth, natural trajectories.

\subsection{Reward Function ($R$)}
A dense reward function guides the agent:
\begin{equation}
R_t = R_{goal} + R_{progress} + R_{collision} + R_{time}
\end{equation}
\begin{itemize}
    \item $R_{goal} = +200$: Reaching the target.
    \item $R_{progress} = 5 \times (d_{t-1} - d_t)$: Reward for moving closer to goal.
    \item $R_{collision} = -100$: Colliding with a wall.
    \item $R_{time} = -0.1$: Penalty per timestep to encourage speed.
\end{itemize}

\subsection{Network Architecture}
We utilize the Actor-Critic architecture of TD3:
\subsubsection{Actor ($\pi_\phi$)}
Input: 28D State. Structure: FC(400, ReLU) $\rightarrow$ FC(300, ReLU) $\rightarrow$ Output(2, Tanh). The output is scaled to the robot's velocity limits.
\subsubsection{Critic ($Q_{\theta_1}, Q_{\theta_2}$)}
Two identical networks estimating the Q-value. Input: Concatenated State + Action. Structure: FC(400) $\rightarrow$ FC(300) $\rightarrow$ Output(1, Linear).

\subsection{Teacher-Student Training (DAGGER)}
One of the primary challenges we faced was the robot spinning in circles during early training. To resolve this, we implemented a form of Dataset Aggregation (DAGGER):
\begin{enumerate}
    \item \textbf{Teacher Phase ($0 - T_k$ steps):} A hard-coded geometric controller drives the robot. It uses a simple heuristic: ``Move towards goal vector; if obstacle $< 0.5m$, turn away.''
    \item \textbf{Data Collection:} These high-quality $(s, a, r, s')$ tuples are stored in the Replay Buffer.
    \item \textbf{Student Phase ($> T_k$ steps):} The TD3 agent takes control. It samples from the buffer (which is rich in successful examples) to update its weights. This acts as ``behavior cloning'' initialization.
\end{enumerate}

\section{System Implementation}

\subsection{ROS 2 \& Gazebo}
The system was implemented in ROS 2 Humble. We utilized the TurtleBot3 simulation package in Gazebo.
\begin{itemize}
    \item \textbf{Vision Node:} A dedicated node processes the camera feed using OpenCV to detect red contours and publish the relative vector to the center of the exit.
    \item \textbf{Synchronization:} A custom Gym-wrapper (\texttt{MazeEnv}) synchronizes the asynchronous ROS callbacks (Laser, Odom, Camera) into a single synchronous step for the DRL agent.
\end{itemize}

\subsection{Safety Layer}
A critical component for deployment is the Safety Layer. This layer intercepts actions from the Agent before they reach the motors.
\begin{equation}
a_{safe} = 
\begin{cases} 
[0, \omega_{turn}] & \text{if } \min(L_{front}) < 0.18m \\
a_{agent} & \text{otherwise}
\end{cases}
\end{equation}
During our development, we identified a critical bug where the agent ignored front obstacles. Analysis revealed the LiDAR indices being checked corresponded to the \textit{rear} of the robot in the ROS coordinate frame. Correcting this to check indices $[0, 1, 23]$ (Front) reduced collision rates by 95\%.

\section{Experimental Results}

\subsection{Training Performance}
The model was trained for 50 episodes (approx 100,000 steps).
\begin{figure}[htbp]
\centerline{\includegraphics[width=0.4\textwidth]{placeholder_reward_graph.png}}
\caption{Average Reward per Episode. The jump at Episode 10 marks the transition from random exploration to Teacher-Guided learning.}
\label{fig:reward}
\end{figure}

The Teacher-Student approach allowed the agent to achieve positive rewards almost immediately, avoiding the thousands of failed episodes typical of ``training from scratch''.

\subsection{Navigation Metrics}
We compared our TD3 agent against a standard DWA planner.
\begin{table}[htbp]
\caption{Performance Comparison}
\begin{center}
\begin{tabular}{|c|c|c|c|}
\hline
\textbf{Method} & \textbf{Success Rate} & \textbf{Avg Speed} & \textbf{Time to Goal} \\
\hline
DWA (Standard) & 100\% & 0.15 m/s & 45.2 s \\
\hline
\textbf{TD3 (Ours)} & \textbf{100\%} & \textbf{0.45 m/s} & \textbf{18.4 s} \\
\hline
\end{tabular}
\label{tab1}
\end{center}
\end{table}

The TD3 agent learned to ``cut corners'' efficiently and maintain a higher velocity (0.45 m/s) compared to the conservative DWA planner. The agent demonstrated robust recovery behaviors, backing up and turning when trapped in dead ends.

\section{Conclusion}
This study demonstrated the efficacy of Deep Reinforcement Learning for mobile robot navigation. By integrating TD3 with a Vision-based goal vector and a Teacher-Student curriculum, we developed a system capable of high-speed, collision-free navigation. Future work will focus on deploying this model to a physical TurtleBot3 Burger and incorporating LSTM layers to handle dynamic obstacles.

\section*{Acknowledgment}
The author thanks the Robotics Team for the collaborative development of the simulation infrastructure.

\begin{thebibliography}{00}
\bibitem{b1} S. Thrun et al., "Probabilistic Robotics," MIT Press, 2005.
\bibitem{b2} V. Mnih et al., "Human-level control through deep reinforcement learning," Nature, 2015.
\bibitem{b3} L. Tai et al., "Virtual-to-real deep reinforcement learning: Continuous control of mobile robots for mapless navigation," IROS, 2017.
\bibitem{b4} S. Fujimoto et al., "Addressing function approximation error in actor-critic methods," ICML, 2018.
\end{thebibliography}

\end{document}
